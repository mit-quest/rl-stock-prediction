{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Trading using Reinforcement Learning and Supervised Learning\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Why Reinforcement Learning over Supervised Learning?\n",
    "\n",
    "Reinforcement learning is a branch of ML which is designed around performing specific action to maximize reward in a given situation. We train our model to optimize, rather than to predict. We want to develop an intelligent agent that will predict stock prices such that a trader will buy at a low price and sell at a high price. In supervised learning, we teach our model to predict future stock prices, by analyzing stock prices in the past. The benefits of Reinforcement Learning become clear in something like stock prediction because of the variability of the stock market; stock fluctuations are incredibly hard to predict simply by anaylizing previous data. In supervised learning the training data set contains all the answers; our model can only be as good as the data we train it on. In reinforcement learning, there is no explicit answer but rather, the agent attempts to decide the best course of action to take given its current position.\n",
    "\n",
    "As good as it sounds though, using reinforcement learning to predict stock pricing is also very difficult as a result of the several different paramaters affecting stock pricing;current number of stocks, recent historical prices, latency, and the available budget to be invested for buying and selling.\n",
    "\n",
    "Through our previous two notebooks, our goal was to see how these two models compared. We belived that RL might be a better alternative because of the flexibility it allows to optimize for unprecendented events. In this notebook, we hope to compare our findings in using our model over five different stocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning Results\n",
    "\n",
    "In our test datasets, our SL model was only really able to accuratley predict the IBM stock. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![[IBM Predictions]](https://imgur.com/j1TKnTz.png \"photo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatley, for the other stocks, based off of our training dataset, we were unable to predict the fluctuations as clearly.\n",
    "\n",
    "![MSFT Predictions](https://imgur.com/lBYm7cC.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, using our profit calculating functions, we generally hovered around 0 profits for the stocks.\n",
    "\n",
    "![MSFT and Google Profits](https://imgur.com/YmqMU0X.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning Results\n",
    "\n",
    "For the Reinforcement Learning model, we were able to optimize our model to perform the three actions of buying , selling and holding accordingly. \n",
    "\n",
    "Looking at the results below, the model was able to make decisions accordingly to the highs and lows of the stock fluctuations. The green dots represent buy actions, and the red dots represent sell actions.\n",
    "\n",
    "![MSFT RL actions](https://imgur.com/cFil8JZ.png)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although promising, the results of the RL model show the model still needs further tweaking to make the most optimal decisions. In the Google stock below, the model consistenly bought at the highest prices, and sold when the price was lowest.\n",
    "\n",
    "![Google RL actions](https://imgur.com/hpsGGW5.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Although the Reinforcement model from a high-level standpoint is extremley promising, it is severly limited by the variability of the environment and the observations it makes. In general, the  RL model suffers because the problem  it tries to solve is inherently difficult, and there is no right answer. The balance of exploration vs. exploitation is extremely difficult; there is always a trade-off to be made. Ideally, we could create an optimal model, if we could visit every possible state and action is tried repeatedly. As we explore and collect more data our your estimates and updates to our Q table continue to change, and the only way to improve is to continue exploring, however at the same time, we are trying to create and optimal policy, given our currently explored states. As a result, the model yields wildly inaccurate results, which necessitate further investigation.\n",
    "\n",
    "Unlike supervised learning, the answer we look for in our policy is not fixed; our optimum is limitless, but the states and actions we need to take to reach that optimum is also limitless. Even with the same datasets, we can expect wildly different results using the same algorithm. With supervised learning however, our results are consistent. Using a supervised learning model may simplify the problem to the variables consider when training the data, but it is still able to perform in a controlled state, so for a complex problem it performs better than an experimental, raw, RL model. In the end, we are very hopeful for the future of Reinforcement Learning and its future ability to be able to mimic the way humans learn. We hope we can continue to improve its performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
