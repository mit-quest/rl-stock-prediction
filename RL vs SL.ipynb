{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Trading using Reinforcement Learning and Supervised Learning\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Why Reinforcement Learning over Supervised Learning?\n",
    "\n",
    "Reinforcement learning is a branch of ML which is designed around performing specific action to maximize reward in a given situation. We train our model to optimize, rather than to predict. We want to develop an intelligent agent that will predict stock prices such that a trader will buy at a low price and sell at a high price. In supervised learning, we teach our model to predict future stock prices, by analyzing stock prices in the past. The benefits of Reinforcement Learning become clear in something like stock prediction because of the variability of the stock market; stock fluctuations are incredibly hard to predict simply by anaylizing previous data. In supervised learning the training data set contains all the answers; our model can only be as good as the data we train it on. In reinforcement learning, there is no explicit answer but rather, the agent attempts to decide the best course of action to take given its current position.\n",
    "\n",
    "As good as it sounds though, using reinforcement learning to predict stock pricing is also very difficult as a result of the several different paramaters affecting stock pricing;current number of stocks, recent historical prices, latency, and the available budget to be invested for buying and selling.\n",
    "\n",
    "Through our previous two notebooks, our goal was to see how these two models compared. We belived that RL might be a better alternative because of the flexibility it allows to optimize for unprecendented events. In this notebook, we hope to compare our findings in using our model over five different stocks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Quick Note\n",
    "\n",
    "Both models were given infinite buying power yet could buy at most one stock per day. It is also important to note the following prices for the used stocks as of December 2019. Microsoft ~ 151.14, Google ~ 1,343.74, Amazon ~ 1,738.36, IBM ~ 133.90, Apple ~ 269.35, and the model was able to perform over a year and a half.\n",
    "\n",
    "Also, if you want to play around with different data for either the Reinforcement Learning or Supervised Learning models, you can get that data from the \"Extra Stock Data\" folder, and make sure you put the csv file you want to test in the \"Stocks\" directory in either the RL or SL model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning Results\n",
    "\n",
    "In our test datasets, our SL model was only really able to accuratley predict the IBM stock. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![[IBM Predictions]](https://imgur.com/j1TKnTz.png \"photo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatley, for the other stocks, based off of our training dataset, we were unable to predict the fluctuations as clearly.\n",
    "\n",
    "![MSFT Predictions](https://imgur.com/lBYm7cC.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, using our profit calculating functions, we generally hovered around 0 profits for the stocks.\n",
    "\n",
    "![MSFT and Google Profits](https://imgur.com/YmqMU0X.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SL model, though was able to generate profits for the IBM and Apple stocks.\n",
    "\n",
    "\n",
    "![IBM and Apple Profits](https://imgur.com/eJ7uYce.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The profits of the SL model for each stock are represented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(51.08000000000004, 'MSFT'),\n",
       " (1073.0800000000013, 'GOOGL'),\n",
       " (276.08000000000266, 'AMZN'),\n",
       " (160.38999999999993, 'IBM'),\n",
       " (179.29999999999976, 'AAPL')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(51.08000000000004, 'MSFT'), (1073.0800000000013, 'GOOGL'), (276.08000000000266, 'AMZN'), (160.38999999999993, 'IBM'), (179.29999999999976, 'AAPL')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning Results\n",
    "\n",
    "For the Reinforcement Learning model, we were able to optimize our model to perform the three actions of buying , selling and holding accordingly. \n",
    "\n",
    "Looking at the results below, the model was able to make decisions accordingly to the highs and lows of the stock fluctuations. The green dots represent buy actions, and the red dots represent sell actions.\n",
    "\n",
    "![MSFT RL actions](https://imgur.com/cFil8JZ.png)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although promising, the results of the RL model show the model still needs further tweaking to make the most optimal decisions. In the Google stock below, the model consistenly bought at the highest prices, and sold when the price was lowest.\n",
    "\n",
    "![Google RL actions](https://imgur.com/hpsGGW5.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end though, our RL model  yielded profits that generally surpassed those of the SL model. The model yielded the following results in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(83.26000000000003, 'MSFT'),\n",
       " (2390.5099999999984, 'GOOGL'),\n",
       " (5752.950000000001, 'AMZN'),\n",
       " (137.88000000000005, 'IBM'),\n",
       " (264.60999999999945, 'AAPL')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "[(83.26000000000003, 'MSFT'), (2390.5099999999984, 'GOOGL'), (5752.950000000001, 'AMZN'), (137.88000000000005, 'IBM'), (264.60999999999945, 'AAPL')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In general, our RL model performed better than the SL model. We believe for multi-faceted problems, it is able to perform better in a highly-variable environment. We would also like to address a few flaws of using Reinforcement Learning for a complex problem like stock prediction.\n",
    "\n",
    "Although the Reinforcement model from a high-level standpoint is extremley promising, it can be limited by the variability of the environment and the observations it makes. In general, the  RL model can suffer because there is no right answer to the problem it address.The balance of exploration vs. exploitation can be extremely difficult; there is always a trade-off to be made. Ideally, we could create an optimal model, if we could visit every possible state and action is tried repeatedly. As we explore and collect more data our your estimates and updates to our Q table continue to change, and the only way to improve is to continue exploring, however at the same time, we are trying to create and optimal policy, given our currently explored states. \n",
    "\n",
    "Unlike supervised learning, the answer we look for in our policy is not fixed; our optimum is limitless, but the states and actions we need to take to reach that optimum are also limitless. Even with the same datasets, we can expect wildly different results using the same algorithm. With supervised learning however, our results are somewhat more consistent. Using a supervised learning model may simplify the problem to the variables consider when training the data, but it is still able to perform in a controlled state, so for a simple problem it performs well. Our RL model, which was able to learn ended  up surpassing the SL model in performance for this problem. In the end, we are very hopeful for the future of Reinforcement Learning and its future ability to be able to mimic the way humans learn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
