{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised Learning is a practical Machine Learning approach where a model is trained to learn the relationship between an input and an output. In supervised learning we first start with a data set containing training examples with associated correct labels. In our case, this is the stock data that we are using. Our supervised learning takes in hundreds of values of stock pricing for a given day, and is then trained to learn the relationship between stock price per day. More formally, weâ€™d like our model to approximate the relationship, lets call it f, between the number of days X and corresponding to stock price.\n",
    "\n",
    "In our approach, there are two main factors we consider. The first is analyzing future profitability on the basis of current performance. (i.e. given our current earnings, how can we decide to buy and or sell?). Secondly, how can we use a statistical analysis to identify trends of the market and make beneficial predictions for the next day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras import optimizers\n",
    "from keras.callbacks import CSVLogger\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import os\n",
    "\n",
    "TIME_STEPS = 30\n",
    "BATCH_SIZE = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Stock Data\n",
    "\n",
    "We start by reading five major stocks, Microsft, Google,  Amazon, IBM, and Apple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "msft = pd.read_csv(\"./Individual_Stock_Data/MSFT.csv\")\n",
    "googl = pd.read_csv(\"./Individual_Stock_Data/GOOGL.csv\")\n",
    "amzn = pd.read_csv(\"./Individual_Stock_Data/AMZN.csv\")\n",
    "ibm = pd.read_csv(\"./Individual_Stock_Data/IBM.csv\")\n",
    "aapl = pd.read_csv(\"./Individual_Stock_Data/AAPL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "msft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(msft[\"Open\"])\n",
    "plt.plot(msft[\"Close\"])\n",
    "plt.plot(msft[\"High\"])\n",
    "plt.plot(msft[\"Low\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Data\n",
    "\n",
    "Normlaizing the data simply helps the algorithm in converging i.e. to find local/ global minimum efficiently, for future predictions. Before fully normalizing the data, we also split the dataset into training and testing data sets. As expected, the training dataset is used to train the model, and the testing dataset is used to observe the performance of the model after the training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(msft, train_size=0.84, test_size=0.16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training and Testing the Data. \n",
    "\n",
    "As part of the normalization mentioned earlier, we transofrm the data, intialize th emodel, and train and test the model. We run the function here but each major part is broken down in the following parts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(data, stock_name=\"\", load_model=False):\n",
    "    df_train, df_test = train_test_split(data, train_size=0.84, test_size=0.16, shuffle=False)\n",
    "    \n",
    "    train_cols = [\"Open\", \"High\", \"Low\", \"Adj Close\", \"Volume\"]\n",
    "    print(\"Train and Test Size:\", len(df_train), len(df_test))\n",
    "    x = df_train.loc[:,train_cols].values\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    \n",
    "    # Transforming Data\n",
    "    x_train = min_max_scaler.fit_transform(x)\n",
    "    x_test = min_max_scaler.transform(df_test.loc[:,train_cols])\n",
    "    print(df_train[train_cols])\n",
    "\n",
    "    \n",
    "\n",
    "    def build_timeseries(mat, y_col_index):\n",
    "        # y_col_index is the index of column that would act as output column\n",
    "        # total number of time-series samples would be len(mat) - TIME_STEPS\n",
    "        dim_0, dim_1 = mat.shape[0] - TIME_STEPS, mat.shape[1]\n",
    "        x = np.zeros((dim_0, TIME_STEPS, dim_1))\n",
    "        y = np.zeros((dim_0,))\n",
    "        for i in range(dim_0):\n",
    "            x[i] = mat[i:TIME_STEPS+i]\n",
    "            y[i] = mat[TIME_STEPS+i, y_col_index]\n",
    "        print(\"length of time-series i/o\",x.shape,y.shape)\n",
    "        return x, y\n",
    "\n",
    "    def trim_dataset(mat, batch_size):\n",
    "        \"\"\"\n",
    "        trims dataset to a size that's divisible by BATCH_SIZE\n",
    "        \"\"\"\n",
    "        no_of_rows_drop = mat.shape[0]%batch_size\n",
    "        if(no_of_rows_drop > 0):\n",
    "            return mat[:-no_of_rows_drop]\n",
    "        else:\n",
    "            return mat\n",
    "\n",
    "    x_t, y_t = build_timeseries(x_train, 3)\n",
    "    x_t = trim_dataset(x_t, BATCH_SIZE)\n",
    "    y_t = trim_dataset(y_t, BATCH_SIZE)\n",
    "    x_temp, y_temp = build_timeseries(x_test, 3)\n",
    "    x_val, x_test_t = np.split(trim_dataset(x_temp, BATCH_SIZE),2)\n",
    "    y_val, y_test_t = np.split(trim_dataset(y_temp, BATCH_SIZE),2)\n",
    "    # print(\"Test size\", x_test_t.shape, y_test_t.shape, x_val.shape, y_val.shape)\n",
    "    \n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(LSTM(100, batch_input_shape=(BATCH_SIZE, TIME_STEPS, x_t.shape[2]), dropout=0.0, recurrent_dropout=0.0, stateful=True,     kernel_initializer='random_uniform'))\n",
    "    lstm_model.add(Dropout(0.4))\n",
    "    lstm_model.add(Dense(40,activation='relu')) # 40\n",
    "    lstm_model.add(Dense(1,activation='sigmoid')) # \n",
    "    optimizer = optimizers.RMSprop(lr=0.001)\n",
    "    lstm_model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    \n",
    "    csv_logger = CSVLogger(\"stock_supervised.log\", append=True)\n",
    "\n",
    "    history = lstm_model.fit(x_t, y_t, epochs=50, verbose=2, batch_size=BATCH_SIZE, #50\n",
    "                            shuffle=False, validation_data=(trim_dataset(x_val, BATCH_SIZE),\n",
    "                            trim_dataset(y_val, BATCH_SIZE)), callbacks=[csv_logger])\n",
    "    \n",
    "    lstm_model.save(\"SL_stock_model.h5\")\n",
    "    \n",
    "    from keras.models import load_model\n",
    "\n",
    "    y_pred = lstm_model.predict(trim_dataset(x_test_t, BATCH_SIZE), batch_size=BATCH_SIZE)\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_test_t = trim_dataset(y_test_t, BATCH_SIZE)\n",
    "    error = mean_squared_error(y_test_t, y_pred)\n",
    "    #print(\"Error is\", error, y_pred.shape, y_test_t.shape)\n",
    "\n",
    "    # convert the predicted value to range of real data\n",
    "    y_pred_org = (y_pred * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3]\n",
    "    # min_max_scaler.inverse_transform(y_pred)\n",
    "    y_test_t_org = (y_test_t * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3]\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(y_pred_org)\n",
    "    plt.plot(y_test_t_org)\n",
    "    plt.title('Prediction vs Real Stock Price ' + stock_name)\n",
    "    plt.ylabel('Price')\n",
    "    plt.xlabel('Days')\n",
    "    plt.legend(['Prediction', 'Real'], loc='upper left')\n",
    "    plt.savefig('pred_vs_real_BS_'+stock_name+'.png')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return y_test_t_org, y_pred_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Time-Series Structure\n",
    "\n",
    "One of the most improtant aspects of creating an effective model is the actual algorithm we use. Here, we are using an LSTM model specifically for its well-suitment to process and predict time series given values that change over an unkown period of time. \n",
    "\n",
    "LSTMs consume input in format of a three-dimensional array; [ batch_size, time_steps, Features ]\n",
    "\n",
    "Batch Size is a number representing the number of samples of data the nueral net should see before re-adjusting and updating weights. \n",
    "\n",
    "The Time Steps basically define how many units back in time you want your network to see.\n",
    "\n",
    "\n",
    "Features is the number of attributes used to represent each time step.\n",
    "\n",
    "We label these parameters in the initializtion of the project.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Converting Data to Time Series\n",
    "\n",
    "In this step, our goal is to convert our data into a format to pass into our model. We want our model to look back on our 'time_step' amount of days and want to represent our data set accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Converting Data to Time Series\n",
    "In this step, our goal is to convert our data into a format to pass into our model. We want our model to look back on our 'time_step' amount of days and want to represent our data set accordingly.\n",
    "\n",
    "### Trimming the Dataset\n",
    "After converting our data, we also want to trim the data to remove any excess odd samples. For example, if we have 51 samples and our batchsize is 25, we would need to trim the excess sample data.\n",
    "\n",
    "### Forming Datasets\n",
    "In this step, we simply form our train, validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_timeseries(mat, y_col_index):\n",
    "    # y_col_index is the index of column that would act as output column\n",
    "    # total number of time-series samples would be len(mat) - TIME_STEPS\n",
    "    dim_0, dim_1 = mat.shape[0] - TIME_STEPS, mat.shape[1]\n",
    "    x = np.zeros((dim_0, TIME_STEPS, dim_1))\n",
    "    y = np.zeros((dim_0,))\n",
    "    for i in range(dim_0):\n",
    "        x[i] = mat[i:TIME_STEPS+i]\n",
    "        y[i] = mat[TIME_STEPS+i, y_col_index]\n",
    "    print(\"length of time-series i/o\",x.shape,y.shape)\n",
    "    return x, y\n",
    "\n",
    "def trim_dataset(mat, batch_size):\n",
    "    \"\"\"\n",
    "    trims dataset to a size that's divisible by BATCH_SIZE\n",
    "    \"\"\"\n",
    "    no_of_rows_drop = mat.shape[0]%batch_size\n",
    "    if(no_of_rows_drop > 0):\n",
    "        return mat[:-no_of_rows_drop]\n",
    "    else:\n",
    "        return mat\n",
    "    \n",
    "x_t, y_t = build_timeseries(x_train, 3)\n",
    "x_t = trim_dataset(x_t, BATCH_SIZE)\n",
    "y_t = trim_dataset(y_t, BATCH_SIZE)\n",
    "x_temp, y_temp = build_timeseries(x_test, 3)\n",
    "x_val, x_test_t = np.split(trim_dataset(x_temp, BATCH_SIZE),2)\n",
    "y_val, y_test_t = np.split(trim_dataset(y_temp, BATCH_SIZE),2)\n",
    "print(\"Test size\", x_test_t.shape, y_test_t.shape, x_val.shape, y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(100, batch_input_shape=(BATCH_SIZE, TIME_STEPS, x_t.shape[2]), dropout=0.0, recurrent_dropout=0.0, stateful=True,     kernel_initializer='random_uniform'))\n",
    "lstm_model.add(Dropout(0.4))\n",
    "lstm_model.add(Dense(20,activation='relu'))\n",
    "lstm_model.add(Dense(1,activation='sigmoid'))\n",
    "optimizer = optimizers.RMSprop(lr=0.001)\n",
    "lstm_model.compile(loss='mean_squared_error', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger(\"stock_supervised.log\", append=True)\n",
    "\n",
    "history = lstm_model.fit(x_t, y_t, epochs=50, verbose=2, batch_size=BATCH_SIZE, #50\n",
    "                        shuffle=False, validation_data=(trim_dataset(x_val, BATCH_SIZE),\n",
    "                        trim_dataset(y_val, BATCH_SIZE)), callbacks=[csv_logger])\n",
    "lstm_model.save(\"SL_stock_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating, Training, and Testing the Model\n",
    "\n",
    "Here, we initialize the LSTM Model and train it using our datasets. Then we test it on our test dataset and plot its prediction trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "y_pred = lstm_model.predict(trim_dataset(x_test_t, BATCH_SIZE), batch_size=BATCH_SIZE)\n",
    "y_pred = y_pred.flatten()\n",
    "y_test_t = trim_dataset(y_test_t, BATCH_SIZE)\n",
    "error = mean_squared_error(y_test_t, y_pred)\n",
    "print(\"Error is\", error, y_pred.shape, y_test_t.shape)\n",
    "\n",
    "# convert the predicted value to range of real data\n",
    "y_pred_org = (y_pred * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3]\n",
    "# min_max_scaler.inverse_transform(y_pred)\n",
    "y_test_t_org = (y_test_t * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3]\n",
    "\n",
    "print(y_pred_org[0:15])\n",
    "print(y_test_t_org[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(y_pred_org)\n",
    "plt.plot(y_test_t_org)\n",
    "plt.title('Prediction vs Real Stock Price')\n",
    "plt.ylabel('Price')\n",
    "plt.xlabel('Days')\n",
    "plt.legend(['Prediction', 'Real'], loc='upper left')\n",
    "plt.show()\n",
    "plt.savefig('pred_vs_real_BS_.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Judging How Effective Our Model Is\n",
    "\n",
    "In the following sections, we plot and detail the performance of our model. We also discuss the changes introduced by shifting the value of episolon or the threshhold for our model to buy or sell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def coeff_determination(y_true, y_pred):\n",
    "\n",
    "    SS_res =K.sum(K.square( y_true-y_pred ))\n",
    "    SS_tot = K.sum(K.square( y_true - np.mean(y_true) ) )\n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "R_2 = coeff_determination(y_test_t_org, y_pred_org)\n",
    "with tf.Session() as sess:  \n",
    "    print(R_2.eval()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred_org\n",
    "y = y_test_t_org\n",
    "positions = [y[0]]\n",
    "profits = []\n",
    "max_len_p = 0\n",
    "eps = 0.02 # Vary this between 0.01 and 0.05 to maximize profit\n",
    "epsilons = np.arange(0, 0.05, .001)\n",
    "max_pos = []\n",
    "for eps in epsilons:\n",
    "    profit= 0\n",
    "    for t in range(1,len(y)):\n",
    "        if y_pred[t] > y[t-1] + eps*y[t-1]:\n",
    "            positions.append(y[t])\n",
    "        elif y_pred[t] < y[t-1] - eps*y[t-1]:\n",
    "            for p in positions:\n",
    "                profit += (y[t] - p)\n",
    "            positions = []\n",
    "        max_len_p = max(max_len_p, len(positions))\n",
    "    profits.append(profit)\n",
    "    max_pos.append(max_len_p)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epsilons, profits)\n",
    "plt.title('Profit maximization')\n",
    "plt.ylabel('Profits')\n",
    "plt.xlabel('Epsilon')\n",
    "plt.legend(['Profits', 'Epsilon'], loc='upper left')\n",
    "plt.show()\n",
    "print(\"Max Profits:\", max(profits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we change the epsilon value, we essentially change the range we can choose whether or not to buy or sell at, rather than hold. From these results, we can see that our maximum epsilon value is around 0.02, so we buy a stock when our predicted price is more than 2% higher than what it was the day before, and we sell if our maximum epsilon value is around 2% lower.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "msft_y, msft_y_pred = train_and_test(msft, \"MSFT\")\n",
    "googl_y, googl_y_pred = train_and_test(googl, \"GOOGL\")\n",
    "amzn_y, amzn_y_pred = train_and_test(amzn, \"AMZN\")\n",
    "ibm_y, ibm_y_pred = train_and_test(ibm, \"IBM\")\n",
    "aapl_y, aapl_y_pred = train_and_test(aapl, \"AAPL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}